---
layout: post
title:  "Paper Analysis: Deep Gradient Compression"
date:   2018-06-05 12:09:00 +0900
tag: [Deep Learning]
---

## [DEEP GRADIENT COMPRESSION: REDUCING THE COMMUNICATION BANDWIDTH FOR DISTRIBUTED TRAINING](https://arxiv.org/abs/1712.01887)


---

ABSTRACT

---

**Large-scale distributed training** requires significant communication bandwidth for **gradient exchange** that limits the scalability of **multi-node training**, and requires expensive high-bandwidth network infrastructure.

>**Large-scale distributed training** 에서는 gradient를 교환하기 위해 상당한 **communication bandwidth** 가 필요하다. 이것은 여러 노드를 이용한 training 을 확장을 제한 하고 네트워크 비용을 증가시킨다. 

---

The situation gets even worse with distributed training on mobile devices (federated learning), which suffers from higher latency, lower throughput, and intermittent poor connections.

>모바일을 이용한 분산 트레이닝에서 (federated learning) 이런 문제는 더 심각해진다. 더 높은 latency, 낮은 throughput, 간헐적 연결장애가 문제가 된다.

---

In this paper, we find 99.9% of the gradient exchange in distributed SGD are redundant, and propose Deep Gradient Compression (DGC) to greatly reduce the communication bandwidth.

>이 논문에서는 분산 SGD 트레이닝에ㅐ서 gradient를 교환할 때 99.9% 는 redundant(중복, 잉여) 라는 것을 확인한다. 그리고 communication bandwidth를 획기적으로 줄여주는 Deep Gradient Compression 를 제안하려 한다.

---

To preserve accuracy during this compression, DGC employs four methods: **momentum correction**, **local gradient clipping**, **momentum factor masking**, and **warm-up training**.

>정확성을 보존하기 위해서, DGC는 4가지 방법을 도입한다.
1. momentum correction
2. local gradient clipping
3. momentum factor masking  
4. warm-up training

---

We have applied Deep Gradient Compression to image classification, speech recognition, and language modeling with multiple datasets including Cifar10, ImageNet, Penn Treebank, and Librispeech Corpus. 

> DGC를 이미지 분류 트레이닝, speech recognition, language modeling에 적용시켰다.

---

On these scenarios, **Deep Gradient Compression** achieves a **gradient compression ratio from 270× to 600× without losing accuracy**, cutting the gradient size of **ResNet-50** from **97MB to 0.35MB**, and for DeepSpeech from **488MB to 0.74MB**. 

> DGC는 정확도를 떨어뜨리지 않고 gradient 압축비율을 270x 에서 600x 로 줄였다. ResNet-50 의 gradient 크기를 97MB 에서 0.35MB 로 줄였고, DeepSpeech 에서는 488MB 에서 0.74MB로 줄였다.

---

Deep gradient compression enables large-scale distributed training on inexpensive commodity 1Gbps Ethernet and facilitates distributed training on mobile.

> DGC는 저가의 상용 1Gbps 이더넷에서 대규모 분산 트레이닝을 가능하게하고 모바일에서 분산 트레이닝을 쉽게 만든다.


---

**SGD** :  **S**tochastic **G**radient **D**escent

